{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGEX\n",
    "**Some valuable regex patterns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='abc'>\n"
     ]
    }
   ],
   "source": [
    "pattern = r'abc'\n",
    "print(re.match(pattern,'abcdefd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='s'>\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\w'\n",
    "print(re.match(pattern,'stranger hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 8), match='stranger'>\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\w+'\n",
    "print(re.match(pattern,'stranger hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='4'>\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\d'\n",
    "print(re.match(pattern,'40 stranger 4 hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 2), match='40'>\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\d+'\n",
    "print(re.match(pattern,'40 stranger 4 hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match=' '>\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\s'\n",
    "print(re.match(pattern,' stranger hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='   '>\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\s+'\n",
    "print(re.match(pattern,'   stranger hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='s'>\n"
     ]
    }
   ],
   "source": [
    "pattern = r'.'\n",
    "print(re.match(pattern,'s   stranger hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match=' '>\n"
     ]
    }
   ],
   "source": [
    "pattern = r'.'\n",
    "print(re.match(pattern,'   stranger hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 13), match='s stranger hi'>\n"
     ]
    }
   ],
   "source": [
    "pattern = r'.*'\n",
    "print(re.match(pattern,'s stranger hi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capital letter '\\S' means not '\\s', in other words NOT SPACE, same with other capital letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 2), match='s '>\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\S\\s'\n",
    "print(re.match(pattern,'s stranger hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\S\\s'\n",
    "print(re.match(pattern,'ssstranger hi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `split` - split a string on regex\n",
    "- `findall` - find all patterns in a string\n",
    "- `search` - search for a pattern\n",
    "- `match` - match an entire string or substring based on a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'stranger', 'hi']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\s'\n",
    "print(re.split(pattern,'s stranger hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '40']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\d+'\n",
    "print(re.findall(pattern,'s 4 stranger 40 hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so. \\\n",
    "            Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "print(re.findall(pattern,my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '             Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String tokenization\n",
    "Turning a string or document into **tokens**(smaller chunks)\n",
    "- One step in preparing a text for NLP models\n",
    "\n",
    "`nltk` - natural language toolkit library for making a simple tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', '!', 'How', 'are', 'you', '?', 'I', \"'m\", 'fine', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"Hi! How are you? I'm fine!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other nltk tokenizers**\n",
    "- `sent_tokenize` - tokenize a document into sentences\n",
    "- `regexp_tokenize` - tokenize a string or document based on a regular expression pattern\n",
    "- `TweetTokenizer` - special class just for tweet tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = 'Highly motivated to develop skills and grow as a Data Science Engineer.\\\n",
    "Technical education (BS or MS in technical field, may be currently pursuing the degree). \\\n",
    "Good knowledge of Computer Science fundamentals: Statistics, Algorithms and Data Structures. \\\n",
    "Basic engineering skills in Python, experience with VCS (preferably Git). \\\n",
    "Basic understanding of Data Science concepts. \\\n",
    "Basic knowledge of relational databases (e.g MySQL). \\\n",
    "Upper-Intermediate English. \\\n",
    "Read at least 1 book a month.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Highly motivated to develop skills and grow as a Data Science Engineer.Technical education (BS or MS in technical field, may be currently pursuing the degree).',\n",
       " 'Good knowledge of Computer Science fundamentals: Statistics, Algorithms and Data Structures.',\n",
       " 'Basic engineering skills in Python, experience with VCS (preferably Git).',\n",
       " 'Basic understanding of Data Science concepts.',\n",
       " 'Basic knowledge of relational databases (e.g MySQL).',\n",
       " 'Upper-Intermediate English.',\n",
       " 'Read at least 1 book a month.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(conditions)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Highly', 'motivated', 'to', 'develop', 'skills', 'and', 'grow', 'as', 'a', 'Data', 'Science', 'Engineer.Technical', 'education', '(', 'BS', 'or', 'MS', 'in', 'technical', 'field', ',', 'may', 'be', 'currently', 'pursuing', 'the', 'degree', ')', '.', 'Good', 'knowledge', 'of', 'Computer', 'Science', 'fundamentals', ':', 'Statistics', ',', 'Algorithms', 'and', 'Data', 'Structures', '.', 'Basic', 'engineering', 'skills', 'in', 'Python', ',', 'experience', 'with', 'VCS', '(', 'preferably', 'Git', ')', '.', 'Basic', 'understanding', 'of', 'Data', 'Science', 'concepts', '.', 'Basic', 'knowledge', 'of', 'relational', 'databases', '(', 'e.g', 'MySQL', ')', '.', 'Upper-Intermediate', 'English', '.', 'Read', 'at', 'least', '1', 'book', 'a', 'month', '.'] \n",
      "\n",
      "{'or', 'knowledge', 'Algorithms', '(', 'VCS', 'field', 'be', 'Computer', 'develop', 'in', 'technical', 'of', 'Basic', 'English', 'month', 'MySQL', ':', 'Upper-Intermediate', 'degree', 'least', 'MS', 'databases', 'e.g', 'at', 'education', 'and', 'currently', ',', 'BS', 'as', 'may', 'pursuing', 'to', 'Highly', 'Good', 'Science', 'motivated', 'engineering', 'experience', 'preferably', 'understanding', 'a', 'Structures', 'Engineer.Technical', '.', 'relational', 'book', 'skills', 'with', 'Statistics', 'concepts', 'fundamentals', 'Python', 'Git', ')', '1', 'Data', 'Read', 'the', 'grow'}\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(conditions)\n",
    "print(words,'\\n')\n",
    "print(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253 258\n"
     ]
    }
   ],
   "source": [
    "match = re.search(\"Basic\", conditions)\n",
    "\n",
    "print(match.start(), match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 19), match='Basic understanding'>\n"
     ]
    }
   ],
   "source": [
    "pattern2 = r\"\\w+\\s\\w+\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex ranges and groups\n",
    "- `[A-Za-z]+` - matches upper and lowercase English alphabet, example:'ABCDEFgkuil'\n",
    "- `[0-9]` - matches numbers from 0 to 9, example: 9\n",
    "- `[A-Za-z\\-\\.]` - matches upper and lowercase English alphabet, - and . example: 'My-Website.com'\n",
    "- `(a-z)` - matches a - and z, example: a-z\n",
    "- `(\\s+|,)` - matches spaces or comma: ','\n",
    "\n",
    "1. **special characters such as: hyphen or period should have `\\` to explain the regex that we want exact character!**\n",
    "2. **`+` at the end meens greedy aproach so it takes all words that matches these conditions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Read', 'at', 'least', '1', 'book', 'a', 'month']\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(sentences[-1],r'\\w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Highly', 'motivated', 'to', 'develop', 'skills', 'and', 'grow', 'as', 'a', 'Data', 'Science', 'Engineer.Technical', 'education', '(', 'BS', 'or', 'MS', 'in', 'technical', 'field', ',', 'may', 'be', 'currently', 'pursuing', 'the', 'degree', ')', '.'], ['Good', 'knowledge', 'of', 'Computer', 'Science', 'fundamentals', ':', 'Statistics', ',', 'Algorithms', 'and', 'Data', 'Structures', '.'], ['Basic', 'engineering', 'skills', 'in', 'Python', ',', 'experience', 'with', 'VCS', '(', 'preferably', 'Git', ')', '.'], ['Basic', 'understanding', 'of', 'Data', 'Science', 'concepts', '.'], ['Basic', 'knowledge', 'of', 'relational', 'databases', '(', 'e', '.', 'g', 'MySQL', ')', '.'], ['Upper-Intermediate', 'English', '.'], ['Read', 'at', 'least', '1', 'book', 'a', 'month', '.']]\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in sentences]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Highly motivated to develop skills and grow as a Data Science Engineer', 'Technical education (BS or MS in technical field, may be currently pursuing the degree)', ' Good knowledge of Computer Science fundamentals: Statistics, Algorithms and Data Structures', ' Basic engineering skills in Python, experience with VCS (preferably Git)', ' Basic understanding of Data Science concepts', ' Basic knowledge of relational databases (e', 'g MySQL)', ' Upper-Intermediate English', ' Read at least 1 book a month', '']\n",
      "[12, 14, 11, 10, 6, 6, 2, 3, 7, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the script into lines: lines\n",
    "lines = conditions.split('.')\n",
    "print(lines)\n",
    "tokenized_lines = [regexp_tokenize(s,'\\w+') for s in lines]\n",
    "\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "print(line_num_words)\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words\n",
    "- basic method for finding topics in a text\n",
    "- Need to first create tokens using tokenization\n",
    "- and then count up all tokens\n",
    "- The theory is, the more frequent a word, the more important it might be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "17363\n",
      "['Президент Володимир Зеленський запросив американського лідера Джо Байдена відвідати Київ і долучитися до саміту \"Кримської платформи\" та святкування 30-річчя Незалежності України. Про повідомили у Міністерстві закордонних справ у відповідь на запит “Радіо Свобода\". \\n', 'Президент Володимир Зеленський заявив на необхідності дієвіших міжнародних кроків для повернення Криму з-під російського контролю Україні. У цьому контексті йдеться про нові санкції за порушення прав людини. Про це глава держави заявив на спільному брифінгу з президентом Європейської Ради Шарлем Мішелем. \\n', 'Верховна рада зробила крок до перезапуску судової реформи: народні депутати підтримали за основу у повторному першому читанні відповідний законопроєкт №3711-д. Про це стало відомо під час засідання Верховної Ради.  \\n', 'Президент Володимир Зеленський прокоментував будівництво \"Північного потоку- 2\", назвавши російський проєкт, у разі добудови, \"четвертим потужним ударом по Україні\". Про це глава держави заявив на спільному брифінгу з президентом Європейської Ради Шарлем Мішелем, котрий перебуває в Україні з офіційним візитом. \\n', 'Верховна Рада ухвалила в першому читанні проєкт закону \"Про обчислення часу в Україні\", яким планується скасувати переведення годинників. \\n']\n"
     ]
    }
   ],
   "source": [
    "with open('tsn.ua.txt') as f:\n",
    "    text = f.readlines()\n",
    "print(type(text))\n",
    "print(len(text))\n",
    "print(text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAT+UlEQVR4nO3cf4xdd5nf8fenNrjpgtmQDMi1ndqAWSmJtgaPXFcURJXdjclW67CCdvLHxlIjGaJEWrSt1KRIJa1kiWzLRopavDJNFAex+VFCGktLuqRhu1ElkzAGE9sJ3kyIdzPYsmcJBa9Y3No8/eN+p70Z3xnb986Pa/n9ko7umeec77nPOYrzuefHvakqJEn6W0vdgCRpOBgIkiTAQJAkNQaCJAkwECRJzfKlbqBfV199da1bt26p25CkS8r+/fv/qqpGei27ZANh3bp1jI+PL3UbknRJSfIXsy3zkpEkCbiAQEjyYJKTSQ511R5LcqBNR5McaPV1Sf6ma9kfdo3ZlORgkokk9ydJq69o25tI8nySdfO/m5Kk87mQM4SHgK3dhar6Z1W1sao2Ak8AX+ta/Or0sqr6dFd9F7AD2NCm6W3eBvy4qt4H3Afc28+OSJIGc95AqKrngDd6LWuf8v8p8Mhc20iyClhZVfuq81sZDwM3t8XbgD1t/qvADdNnD5KkxTPoPYQPAyeq6pWu2vok303yZ0k+3GqrgcmudSZbbXrZ6wBVdQb4CXDVgH1Jki7SoE8Z3cKbzw6OA9dU1Y+SbAL+a5LrgF6f+Kd/VW+uZW+SZAedy05cc801fTctSTpX32cISZYDvw08Nl2rqtNV9aM2vx94FXg/nTOCNV3D1wDH2vwksLZrm+9glktUVbW7qkaranRkpOdjtJKkPg1yyejXgO9X1f+7FJRkJMmyNv8eOjePf1BVx4FTSba0+wO3Ak+1YXuB7W3+E8A3y9/klqRFdyGPnT4C7AN+JclkktvaojHOvZn8EeDFJN+jc4P401U1/Wn/duA/AxN0zhyebvUHgKuSTAC/B9w1wP5IkvqUS/XD+OjoaPX7TeV1d/3xPHdz4Y5+/jeX7L0lKcn+qhrttcxvKkuSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEnNeQMhyYNJTiY51FW7J8kPkxxo001dy+5OMpHkSJIbu+qbkhxsy+5PklZfkeSxVn8+ybp53kdJ0gW4kDOEh4CtPer3VdXGNn0dIMm1wBhwXRvzxSTL2vq7gB3AhjZNb/M24MdV9T7gPuDePvdFkjSA8wZCVT0HvHGB29sGPFpVp6vqNWAC2JxkFbCyqvZVVQEPAzd3jdnT5r8K3DB99iBJWjyD3EO4M8mL7ZLSla22Gni9a53JVlvd5mfW3zSmqs4APwGu6vWGSXYkGU8yPjU1NUDrkqSZ+g2EXcB7gY3AceALrd7rk33NUZ9rzLnFqt1VNVpVoyMjIxfVsCRpbn0FQlWdqKqzVfUL4EvA5rZoEljbteoa4Firr+lRf9OYJMuBd3Dhl6gkSfOkr0Bo9wSmfRyYfgJpLzDWnhxaT+fm8QtVdRw4lWRLuz9wK/BU15jtbf4TwDfbfQZJ0iJafr4VkjwCfBS4Oskk8Dngo0k20rm0cxT4FEBVHU7yOPAScAa4o6rOtk3dTueJpSuAp9sE8ADw5SQTdM4MxuZhvyRJF+m8gVBVt/QoPzDH+juBnT3q48D1Peo/Bz55vj4kSQvLbypLkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAm4gEBI8mCSk0kOddX+fZLvJ3kxyZNJfrnV1yX5myQH2vSHXWM2JTmYZCLJ/UnS6iuSPNbqzydZN/+7KUk6nws5Q3gI2Dqj9gxwfVX9KvDnwN1dy16tqo1t+nRXfRewA9jQpult3gb8uKreB9wH3HvReyFJGth5A6GqngPemFH7RlWdaX9+C1gz1zaSrAJWVtW+qirgYeDmtngbsKfNfxW4YfrsQZK0eObjHsI/B57u+nt9ku8m+bMkH2611cBk1zqTrTa97HWAFjI/Aa7q9UZJdiQZTzI+NTU1D61LkqYNFAhJPgucAb7SSseBa6rqA8DvAX+UZCXQ6xN/TW9mjmVvLlbtrqrRqhodGRkZpHVJ0gzL+x2YZDvwT4Ab2mUgquo0cLrN70/yKvB+OmcE3ZeV1gDH2vwksBaYTLIceAczLlFJkhZeX2cISbYC/wr4rar6WVd9JMmyNv8eOjePf1BVx4FTSba0+wO3Ak+1YXuB7W3+E8A3pwNGkrR4znuGkOQR4KPA1Ukmgc/ReapoBfBMu//7rfZE0UeAf5fkDHAW+HRVTX/av53OE0tX0LnnMH3f4QHgy0km6JwZjM3LnkmSLsp5A6GqbulRfmCWdZ8Anphl2ThwfY/6z4FPnq8PSdLC8pvKkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCLiAQkjyY5GSSQ121dyZ5Jskr7fXKrmV3J5lIciTJjV31TUkOtmX3J0mrr0jyWKs/n2TdPO+jJOkCXMgZwkPA1hm1u4Bnq2oD8Gz7myTXAmPAdW3MF5Msa2N2ATuADW2a3uZtwI+r6n3AfcC9/e6MJKl/5w2EqnoOeGNGeRuwp83vAW7uqj9aVaer6jVgAticZBWwsqr2VVUBD88YM72trwI3TJ89SJIWT7/3EN5dVccB2uu7Wn018HrXepOttrrNz6y/aUxVnQF+AlzV602T7EgynmR8amqqz9YlSb3M903lXp/sa476XGPOLVbtrqrRqhodGRnps0VJUi/9BsKJdhmI9nqy1SeBtV3rrQGOtfqaHvU3jUmyHHgH516ikiQtsH4DYS+wvc1vB57qqo+1J4fW07l5/EK7rHQqyZZ2f+DWGWOmt/UJ4JvtPoMkaREtP98KSR4BPgpcnWQS+BzweeDxJLcBfwl8EqCqDid5HHgJOAPcUVVn26Zup/PE0hXA020CeAD4cpIJOmcGY/OyZ5Kki3LeQKiqW2ZZdMMs6+8EdvaojwPX96j/nBYokqSl4zeVJUmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEDBAISX4lyYGu6adJPpPkniQ/7Krf1DXm7iQTSY4kubGrvinJwbbs/iQZdMckSRen70CoqiNVtbGqNgKbgJ8BT7bF900vq6qvAyS5FhgDrgO2Al9MsqytvwvYAWxo09Z++5Ik9We+LhndALxaVX8xxzrbgEer6nRVvQZMAJuTrAJWVtW+qirgYeDmeepLknSB5isQxoBHuv6+M8mLSR5McmWrrQZe71pnstVWt/mZ9XMk2ZFkPMn41NTUPLUuSYJ5CIQkbwV+C/gvrbQLeC+wETgOfGF61R7Da476ucWq3VU1WlWjIyMjg7QtSZphPs4QPgZ8p6pOAFTViao6W1W/AL4EbG7rTQJru8atAY61+poedUnSIpqPQLiFrstF7Z7AtI8Dh9r8XmAsyYok6+ncPH6hqo4Dp5JsaU8X3Qo8NQ99SZIuwvJBBif5O8CvA5/qKv9+ko10LvscnV5WVYeTPA68BJwB7qiqs23M7cBDwBXA022SJC2igQKhqn4GXDWj9jtzrL8T2NmjPg5cP0gvkqTB+E1lSRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBAwZCkqNJDiY5kGS81d6Z5Jkkr7TXK7vWvzvJRJIjSW7sqm9q25lIcn+SDNKXJOnizccZwj+uqo1VNdr+vgt4tqo2AM+2v0lyLTAGXAdsBb6YZFkbswvYAWxo09Z56EuSdBEW4pLRNmBPm98D3NxVf7SqTlfVa8AEsDnJKmBlVe2rqgIe7hojSVokgwZCAd9Isj/JjlZ7d1UdB2iv72r11cDrXWMnW211m59ZP0eSHUnGk4xPTU0N2LokqdvyAcd/qKqOJXkX8EyS78+xbq/7AjVH/dxi1W5gN8Do6GjPdSRJ/RnoDKGqjrXXk8CTwGbgRLsMRHs92VafBNZ2DV8DHGv1NT3qkqRF1HcgJPmlJG+fngd+AzgE7AW2t9W2A0+1+b3AWJIVSdbTuXn8QrusdCrJlvZ00a1dYyRJi2SQS0bvBp5sT4guB/6oqv5bkm8Djye5DfhL4JMAVXU4yePAS8AZ4I6qOtu2dTvwEHAF8HSbJEmLqO9AqKofAH+/R/1HwA2zjNkJ7OxRHweu77cXSdLg/KayJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJKAAQIhydokf5rk5SSHk/xuq9+T5IdJDrTppq4xdyeZSHIkyY1d9U1JDrZl9yfJYLslSbpYywcYewb4F1X1nSRvB/YneaYtu6+q/kP3ykmuBcaA64C/C/z3JO+vqrPALmAH8C3g68BW4OkBepMkXaS+zxCq6nhVfafNnwJeBlbPMWQb8GhVna6q14AJYHOSVcDKqtpXVQU8DNzcb1+SpP7Myz2EJOuADwDPt9KdSV5M8mCSK1ttNfB617DJVlvd5mfWJUmLaOBASPI24AngM1X1UzqXf94LbASOA1+YXrXH8Jqj3uu9diQZTzI+NTU1aOuSpC4DBUKSt9AJg69U1dcAqupEVZ2tql8AXwI2t9UngbVdw9cAx1p9TY/6Oapqd1WNVtXoyMjIIK1LkmYY5CmjAA8AL1fVH3TVV3Wt9nHgUJvfC4wlWZFkPbABeKGqjgOnkmxp27wVeKrfviRJ/RnkKaMPAb8DHExyoNX+NXBLko10LvscBT4FUFWHkzwOvETnCaU72hNGALcDDwFX0Hm6yCeMJGmR9R0IVfU/6X39/+tzjNkJ7OxRHweu77cXSdLg/KayJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1AxNICTZmuRIkokkdy11P5J0uVm+1A0AJFkG/Cfg14FJ4NtJ9lbVS0vb2fxbd9cfL8n7Hv38by7J+0q6dAzLGcJmYKKqflBV/xt4FNi2xD1J0mVlKM4QgNXA611/TwL/YOZKSXYAO9qff53kSJ/vdzXwV32OXWzz0mvunYdOzu+yO66L6FLq114Xxnz1+vdmWzAsgZAetTqnULUb2D3wmyXjVTU66HYWg70ujEupV7i0+rXXhbEYvQ7LJaNJYG3X32uAY0vUiyRdloYlEL4NbEiyPslbgTFg7xL3JEmXlaG4ZFRVZ5LcCfwJsAx4sKoOL+BbDnzZaRHZ68K4lHqFS6tfe10YC95rqs65VC9JugwNyyUjSdISMxAkScBlGAjD+BMZSY4mOZjkQJLxVntnkmeSvNJer+xa/+7W/5EkNy5wbw8mOZnkUFftontLsqnt40SS+5P0etR4IXq9J8kP27E9kOSmIel1bZI/TfJyksNJfrfVh+7YztHr0B3bJH87yQtJvtd6/betPozHdbZel+64VtVlM9G5Yf0q8B7grcD3gGuHoK+jwNUzar8P3NXm7wLubfPXtr5XAOvb/ixbwN4+AnwQODRIb8ALwD+k852Tp4GPLVKv9wD/sse6S93rKuCDbf7twJ+3nobu2M7R69Ad27bdt7X5twDPA1uG9LjO1uuSHdfL7QzhUvqJjG3Anja/B7i5q/5oVZ2uqteACTr7tSCq6jngjUF6S7IKWFlV+6rzX+/DXWMWutfZLHWvx6vqO23+FPAynW/sD92xnaPX2Sxlr1VVf93+fEubiuE8rrP1OpsF7/VyC4ReP5Ex13/Yi6WAbyTZn87PcwC8u6qOQ+cfJPCuVh+GfbjY3la3+Zn1xXJnkhfbJaXpSwVD02uSdcAH6HxCHOpjO6NXGMJjm2RZkgPASeCZqhra4zpLr7BEx/VyC4QL+omMJfChqvog8DHgjiQfmWPdYd0HmL23pex5F/BeYCNwHPhCqw9Fr0neBjwBfKaqfjrXqj1qi9pvj16H8thW1dmq2kjnFw82J7l+jtWHsdclO66XWyAM5U9kVNWx9noSeJLOJaAT7VSQ9nqyrT4M+3CxvU22+Zn1BVdVJ9o/ul8AX+L/X15b8l6TvIXO/2C/UlVfa+WhPLa9eh3mY9v6+1/A/wC2MqTHtVevS3lcL7dAGLqfyEjyS0nePj0P/AZwqPW1va22HXiqze8FxpKsSLIe2EDnhtJiuqje2in6qSRb2tMPt3aNWVDT/xNoPk7n2C55r23bDwAvV9UfdC0aumM7W6/DeGyTjCT55TZ/BfBrwPcZzuPas9clPa7zcbf8UpqAm+g8JfEq8Nkh6Oc9dJ4c+B5weLon4CrgWeCV9vrOrjGfbf0fYQGegJnR3yN0Tlv/D51PIrf10xsw2v7DfhX4j7RvyS9Cr18GDgIvtn9Qq4ak139E57T+ReBAm24axmM7R69Dd2yBXwW+23o6BPybfv89LWGvS3Zc/ekKSRJw+V0ykiTNwkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJKa/wve1ZITJPzhqQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_lines = [regexp_tokenize(s,'\\w+') for s in text]\n",
    "\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "#print(line_num_words)\n",
    "\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 128917), ('.', 91318), ('``', 30926), (\"''\", 30179), ('у', 28879), ('на', 28355), ('в', 25969), ('про', 21931), (\"'\", 19508), ('з', 19485)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens = word_tokenize(str(text))\n",
    "\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "example = Counter(lower_tokens)\n",
    "\n",
    "print(example.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('у', 28879), ('на', 28355), ('в', 25969), ('про', 21931), ('з', 19485), ('і', 18646), ('що', 18027), ('це', 17092), ('та', 15294), ('україни', 14699)]\n"
     ]
    }
   ],
   "source": [
    "tokens = [w for w in word_tokenize(str(text).lower()) if w.isalpha()]\n",
    "\n",
    "example1 = Counter(tokens)\n",
    "print(example1.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('з', 19485), ('і', 18646), ('що', 18027), ('це', 17092), ('та', 15294), ('україни', 14699), ('він', 8193), ('від', 5788), ('його', 5416), ('під', 5038)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "no_stop = [t for t in tokens if t not in stopwords.words('russian')]\n",
    "print(Counter(no_stop).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'б',\n",
       " 'в',\n",
       " 'г',\n",
       " 'е',\n",
       " 'ж',\n",
       " 'з',\n",
       " 'м',\n",
       " 'т',\n",
       " 'у',\n",
       " 'я',\n",
       " 'є',\n",
       " 'і',\n",
       " 'аж',\n",
       " 'ви',\n",
       " 'де',\n",
       " 'до',\n",
       " 'за',\n",
       " 'зі',\n",
       " 'ми',\n",
       " 'на',\n",
       " 'не',\n",
       " 'ну',\n",
       " 'нх',\n",
       " 'ні',\n",
       " 'по',\n",
       " 'та',\n",
       " 'ти',\n",
       " 'то',\n",
       " 'ту',\n",
       " 'ті',\n",
       " 'це',\n",
       " 'цю',\n",
       " 'ця',\n",
       " 'ці',\n",
       " 'чи',\n",
       " 'ще',\n",
       " 'що',\n",
       " 'як',\n",
       " 'їй',\n",
       " 'їм',\n",
       " 'їх',\n",
       " 'її',\n",
       " 'або',\n",
       " 'але',\n",
       " 'ало',\n",
       " 'без',\n",
       " 'був',\n",
       " 'вам',\n",
       " 'вас',\n",
       " 'ваш',\n",
       " 'вже',\n",
       " 'все',\n",
       " 'всю',\n",
       " 'вся',\n",
       " 'від',\n",
       " 'він',\n",
       " 'два',\n",
       " 'дві',\n",
       " 'для',\n",
       " 'ким',\n",
       " 'мож',\n",
       " 'моя',\n",
       " 'моє',\n",
       " 'мої',\n",
       " 'міг',\n",
       " 'між',\n",
       " 'мій',\n",
       " 'над',\n",
       " 'нам',\n",
       " 'нас',\n",
       " 'наш',\n",
       " 'нею',\n",
       " 'неї',\n",
       " 'них',\n",
       " 'ніж',\n",
       " 'ній',\n",
       " 'ось',\n",
       " 'при',\n",
       " 'про',\n",
       " 'під',\n",
       " 'пір',\n",
       " 'раз',\n",
       " 'рік',\n",
       " 'сам',\n",
       " 'сих',\n",
       " 'сім',\n",
       " 'так',\n",
       " 'там',\n",
       " 'теж',\n",
       " 'тим',\n",
       " 'тих',\n",
       " 'той',\n",
       " 'тою',\n",
       " 'три',\n",
       " 'тут',\n",
       " 'хоч',\n",
       " 'хто',\n",
       " 'цей',\n",
       " 'цим',\n",
       " 'цих',\n",
       " 'час',\n",
       " 'щоб',\n",
       " 'яка',\n",
       " 'які',\n",
       " 'адже',\n",
       " 'буде',\n",
       " 'буду',\n",
       " 'будь',\n",
       " 'була',\n",
       " 'були',\n",
       " 'було',\n",
       " 'бути',\n",
       " 'вами',\n",
       " 'ваша',\n",
       " 'ваше',\n",
       " 'ваші',\n",
       " 'весь',\n",
       " 'вниз',\n",
       " 'вона',\n",
       " 'вони',\n",
       " 'воно',\n",
       " 'всею',\n",
       " 'всім',\n",
       " 'всіх',\n",
       " 'втім',\n",
       " 'геть',\n",
       " 'далі',\n",
       " 'двох',\n",
       " 'день',\n",
       " 'дуже',\n",
       " 'зате',\n",
       " 'його',\n",
       " 'йому',\n",
       " 'каже',\n",
       " 'кого',\n",
       " 'коли',\n",
       " 'кому',\n",
       " 'крім',\n",
       " 'куди',\n",
       " 'лише',\n",
       " 'люди',\n",
       " 'мало',\n",
       " 'мати',\n",
       " 'мене',\n",
       " 'мені',\n",
       " 'миру',\n",
       " 'мною',\n",
       " 'може',\n",
       " 'нами',\n",
       " 'наша',\n",
       " 'наше',\n",
       " 'наші',\n",
       " 'ними',\n",
       " 'ніби',\n",
       " 'один',\n",
       " 'поки',\n",
       " 'пора',\n",
       " 'рано',\n",
       " 'року',\n",
       " 'році',\n",
       " 'сама',\n",
       " 'саме',\n",
       " 'саму',\n",
       " 'самі',\n",
       " 'свою',\n",
       " 'своє',\n",
       " 'свої',\n",
       " 'себе',\n",
       " 'собі',\n",
       " 'став',\n",
       " 'суть',\n",
       " 'така',\n",
       " 'таке',\n",
       " 'такі',\n",
       " 'твоя',\n",
       " 'твоє',\n",
       " 'твій',\n",
       " 'тебе',\n",
       " 'тими',\n",
       " 'тобі',\n",
       " 'того',\n",
       " 'тоді',\n",
       " 'тому',\n",
       " 'туди',\n",
       " 'хоча',\n",
       " 'хіба',\n",
       " 'цими',\n",
       " 'цієї',\n",
       " 'часу',\n",
       " 'чого',\n",
       " 'чому',\n",
       " 'який',\n",
       " 'яких',\n",
       " 'якої',\n",
       " 'якщо',\n",
       " \"ім'я\",\n",
       " 'інша',\n",
       " 'інше',\n",
       " 'інші',\n",
       " 'буває',\n",
       " 'будеш',\n",
       " 'більш',\n",
       " 'вгору',\n",
       " 'вміти',\n",
       " 'внизу',\n",
       " 'вісім',\n",
       " 'давно',\n",
       " 'даром',\n",
       " 'добре',\n",
       " 'довго',\n",
       " 'друго',\n",
       " 'дякую',\n",
       " 'життя',\n",
       " 'зараз',\n",
       " 'знову',\n",
       " 'какая',\n",
       " 'кожен',\n",
       " 'кожна',\n",
       " 'кожне',\n",
       " 'кожні',\n",
       " 'краще',\n",
       " 'ледве',\n",
       " 'майже',\n",
       " 'менше',\n",
       " 'могти',\n",
       " 'можна',\n",
       " 'назад',\n",
       " 'немає',\n",
       " 'нижче',\n",
       " 'нього',\n",
       " 'однак',\n",
       " \"п'ять\",\n",
       " 'перед',\n",
       " 'поруч',\n",
       " 'потім',\n",
       " 'проти',\n",
       " 'після',\n",
       " 'років',\n",
       " 'самим',\n",
       " 'самих',\n",
       " 'самій',\n",
       " 'свого',\n",
       " 'своєї',\n",
       " 'своїх',\n",
       " 'собою',\n",
       " 'справ',\n",
       " 'такий',\n",
       " 'також',\n",
       " 'тепер',\n",
       " 'тисяч',\n",
       " 'тобою',\n",
       " 'треба',\n",
       " 'трохи',\n",
       " 'усюди',\n",
       " 'усіма',\n",
       " 'хочеш',\n",
       " 'цього',\n",
       " 'цьому',\n",
       " 'часто',\n",
       " 'через',\n",
       " 'шість',\n",
       " 'якого',\n",
       " 'іноді',\n",
       " 'інший',\n",
       " 'інших',\n",
       " 'багато',\n",
       " 'будемо',\n",
       " 'будете',\n",
       " 'будуть',\n",
       " 'більше',\n",
       " 'всього',\n",
       " 'всьому',\n",
       " 'далеко',\n",
       " 'десять',\n",
       " 'досить',\n",
       " 'другий',\n",
       " 'дійсно',\n",
       " 'завжди',\n",
       " 'звідси',\n",
       " 'зовсім',\n",
       " 'кругом',\n",
       " 'кілька',\n",
       " 'людина',\n",
       " 'можуть',\n",
       " 'навіть',\n",
       " 'навіщо',\n",
       " 'нагорі',\n",
       " 'небудь',\n",
       " 'низько',\n",
       " 'ніколи',\n",
       " 'нікуди',\n",
       " 'нічого',\n",
       " 'обидва',\n",
       " 'одного',\n",
       " 'однієї',\n",
       " \"п'ятий\",\n",
       " 'перший',\n",
       " 'просто',\n",
       " 'раніше',\n",
       " 'раптом',\n",
       " 'самими',\n",
       " 'самого',\n",
       " 'самому',\n",
       " 'сказав',\n",
       " 'скрізь',\n",
       " 'сьомий',\n",
       " 'третій',\n",
       " 'тільки',\n",
       " 'хотіти',\n",
       " 'чотири',\n",
       " 'чудово',\n",
       " 'шостий',\n",
       " 'близько',\n",
       " 'важлива',\n",
       " 'важливе',\n",
       " 'важливі',\n",
       " 'вдалині',\n",
       " 'восьмий',\n",
       " 'говорив',\n",
       " \"дев'ять\",\n",
       " 'десятий',\n",
       " 'зайнята',\n",
       " 'зайнято',\n",
       " 'зайняті',\n",
       " 'занадто',\n",
       " 'значить',\n",
       " 'навколо',\n",
       " 'нарешті',\n",
       " 'нерідко',\n",
       " 'повинно',\n",
       " 'посеред',\n",
       " 'початку',\n",
       " 'пізніше',\n",
       " 'сказала',\n",
       " 'сказати',\n",
       " 'скільки',\n",
       " 'спасибі',\n",
       " 'частіше',\n",
       " 'важливий',\n",
       " 'двадцять',\n",
       " \"дев'ятий\",\n",
       " 'зазвичай',\n",
       " 'зайнятий',\n",
       " 'звичайно',\n",
       " 'здається',\n",
       " 'найбільш',\n",
       " 'не можна',\n",
       " 'недалеко',\n",
       " 'особливо',\n",
       " 'потрібно',\n",
       " 'спочатку',\n",
       " 'сьогодні',\n",
       " 'численна',\n",
       " 'численне',\n",
       " 'численні',\n",
       " 'відсотків',\n",
       " 'двадцятий',\n",
       " 'звідусіль',\n",
       " 'мільйонів',\n",
       " 'нещодавно',\n",
       " 'прекрасно',\n",
       " 'четвертий',\n",
       " 'численний',\n",
       " 'будь ласка',\n",
       " 'дванадцять',\n",
       " 'одинадцять',\n",
       " 'сімнадцять',\n",
       " 'тринадцять',\n",
       " 'безперервно',\n",
       " 'дванадцятий',\n",
       " 'одинадцятий',\n",
       " 'одного разу',\n",
       " \"п'ятнадцять\",\n",
       " 'сімнадцятий',\n",
       " 'тринадцятий',\n",
       " 'шістнадцять',\n",
       " 'вісімнадцять',\n",
       " \"п'ятнадцятий\",\n",
       " 'чотирнадцять',\n",
       " 'шістнадцятий',\n",
       " 'вісімнадцятий',\n",
       " \"дев'ятнадцять\",\n",
       " 'чотирнадцятий',\n",
       " \"дев'ятнадцятий\"]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "stop_words = get_stop_words('ukrainian')\n",
    "#stop_words.append('а').apeend('із').append('щодо')\n",
    "stop_words.extend('а','із','щодо','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('з', 19485), ('і', 18646), ('що', 18027), ('це', 17092), ('та', 15294), ('україни', 14699), ('він', 8193), ('від', 5788), ('його', 5416), ('під', 5038)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in stopwords.words('russian')]\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('україни', 14699), ('а', 5073), ('заявив', 4974), ('президента', 4830), ('із', 4332), ('щодо', 4141), ('сша', 3345), ('ради', 3302), ('йдеться', 3234), ('президент', 3076)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in stop_words]\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('ukrainian')\n",
    "type(stop_words)\n",
    "stop_words.append('а')\n",
    "stop_words.append('із')\n",
    "stop_words.append('щодо')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('україни', 14699), ('заявив', 4974), ('президента', 4830), ('сша', 3345), ('ради', 3302), ('йдеться', 3234), ('президент', 3076), ('україні', 2728), ('зеленський', 2673), ('володимир', 2634)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in stop_words]\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('україни', 14699), ('заявив', 4974), ('президента', 4830), ('сша', 3345), ('ради', 3302), ('йдеться', 3234), ('президент', 3076), ('україні', 2728), ('зеленський', 2673), ('володимир', 2634), ('порошенко', 2422), ('рішення', 2413), ('росії', 2286), ('повідомляє', 2142), ('словами', 1939), ('верховної', 1888), ('держави', 1861), ('партії', 1856), ('рф', 1841), ('має', 1826), ('режим', 1772), ('те', 1670), ('повноекранний', 1627), ('повідомив', 1616), ('петро', 1610)]\n"
     ]
    }
   ],
   "source": [
    "print(bow.most_common(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim library\n",
    "- popular open-source NLP library\n",
    "- Uses top academic models to perform comlex tasks\n",
    "    - Building document or word-vectors\n",
    "    - performing topic identification and document comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stop-words in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (2018.7.23)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf\n",
    "- Term frequency - inverse document frequency\n",
    "- Allows to determine the most important words in each document\n",
    "- Each corpus may have shared words beyond the stopwords\n",
    "- These words should be down-weighted in importance\n",
    "- Ensures most common words don't show up as a key words\n",
    "- Keeps document specific frequent words weighted high\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can calculate the weights of tokens with this formula:\n",
    "**Wij = TFij * log(N/DFi)**\n",
    "- Wij = tf-idf weight of token `i` in document `j`\n",
    "- TFij = number of occurences of token i in document `j`\n",
    "- DFi = number of documents that contain token `i`\n",
    "- N = total number of documents\n",
    "\n",
    "### How we can understand this:\n",
    "- first, the weight will be low if the term doesn't appear often in the document\n",
    "- however, the weight will also be low if the logarithm is close to zero, meaning the internal equation is low, so **words that occure across many or all documents will have a very low tf-idf weight, so if N = DFi that N/DFi = 1 and log(1)=0**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with NLTK\n",
    "- Named entity recognition\n",
    "- NLP task to identify important named entities in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe taxi-hailing company'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('News_articles/uber_apple.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')\n",
    "data[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe taxi-hailing company Uber brings into very sharp focus the question of whether corporations can be said to have a moral character.', 'If any human being were to behave with the single-minded and ruthless greed of the company, we would consider them sociopathic.', 'Uber wanted to know as much as possible about the people who use its service, and those who don’t.'] \n",
      "\n",
      "[['\\ufeffThe', 'taxi-hailing', 'company', 'Uber', 'brings', 'into', 'very', 'sharp', 'focus', 'the', 'question', 'of', 'whether', 'corporations', 'can', 'be', 'said', 'to', 'have', 'a', 'moral', 'character', '.'], ['If', 'any', 'human', 'being', 'were', 'to', 'behave', 'with', 'the', 'single-minded', 'and', 'ruthless', 'greed', 'of', 'the', 'company', ',', 'we', 'would', 'consider', 'them', 'sociopathic', '.'], ['Uber', 'wanted', 'to', 'know', 'as', 'much', 'as', 'possible', 'about', 'the', 'people', 'who', 'use', 'its', 'service', ',', 'and', 'those', 'who', 'don', '’', 't', '.']] \n",
      "\n",
      "[[('\\ufeffThe', 'JJ'), ('taxi-hailing', 'JJ'), ('company', 'NN'), ('Uber', 'NNP'), ('brings', 'VBZ'), ('into', 'IN'), ('very', 'RB'), ('sharp', 'JJ'), ('focus', 'VB'), ('the', 'DT'), ('question', 'NN'), ('of', 'IN'), ('whether', 'IN'), ('corporations', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('said', 'VBD'), ('to', 'TO'), ('have', 'VB'), ('a', 'DT'), ('moral', 'JJ'), ('character', 'NN'), ('.', '.')], [('If', 'IN'), ('any', 'DT'), ('human', 'JJ'), ('being', 'VBG'), ('were', 'VBD'), ('to', 'TO'), ('behave', 'VB'), ('with', 'IN'), ('the', 'DT'), ('single-minded', 'JJ'), ('and', 'CC'), ('ruthless', 'JJ'), ('greed', 'NN'), ('of', 'IN'), ('the', 'DT'), ('company', 'NN'), (',', ','), ('we', 'PRP'), ('would', 'MD'), ('consider', 'VB'), ('them', 'PRP'), ('sociopathic', 'JJ'), ('.', '.')], [('Uber', 'NNP'), ('wanted', 'VBD'), ('to', 'TO'), ('know', 'VB'), ('as', 'RB'), ('much', 'JJ'), ('as', 'IN'), ('possible', 'JJ'), ('about', 'IN'), ('the', 'DT'), ('people', 'NNS'), ('who', 'WP'), ('use', 'VBP'), ('its', 'PRP$'), ('service', 'NN'), (',', ','), ('and', 'CC'), ('those', 'DT'), ('who', 'WP'), ('don', 'VBP'), ('’', 'JJ'), ('t', 'NN'), ('.', '.')]] \n",
      "\n",
      "[Tree('S', [('\\ufeffThe', 'JJ'), ('taxi-hailing', 'JJ'), ('company', 'NN'), ('Uber', 'NNP'), ('brings', 'VBZ'), ('into', 'IN'), ('very', 'RB'), ('sharp', 'JJ'), ('focus', 'VB'), ('the', 'DT'), ('question', 'NN'), ('of', 'IN'), ('whether', 'IN'), ('corporations', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('said', 'VBD'), ('to', 'TO'), ('have', 'VB'), ('a', 'DT'), ('moral', 'JJ'), ('character', 'NN'), ('.', '.')]), Tree('S', [('If', 'IN'), ('any', 'DT'), ('human', 'JJ'), ('being', 'VBG'), ('were', 'VBD'), ('to', 'TO'), ('behave', 'VB'), ('with', 'IN'), ('the', 'DT'), ('single-minded', 'JJ'), ('and', 'CC'), ('ruthless', 'JJ'), ('greed', 'NN'), ('of', 'IN'), ('the', 'DT'), ('company', 'NN'), (',', ','), ('we', 'PRP'), ('would', 'MD'), ('consider', 'VB'), ('them', 'PRP'), ('sociopathic', 'JJ'), ('.', '.')])]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Tokenize the article into sentences: sentences\n",
    "sentences = sent_tokenize(data)\n",
    "print(sentences[:3],'\\n')\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "print(token_sentences[:3],'\\n')\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "print(pos_sentences[:3],'\\n')\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)\n",
    "print([i for i in chunked_sentences][:2])\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values\n",
    "values = [ner_categories.get(v) for v in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to SpaCy\n",
    "- NLP library si,ilar to gensim, with different implementations\n",
    "- focus on creating NLP pipelines to generate models and corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 622 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.50.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: setuptools in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: six in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.0.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ru-core-news-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.0.0/ru_core_news_sm-3.0.0-py3-none-any.whl (17.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.9 MB 423 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pymorphy2>=0.9\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from ru-core-news-sm==3.0.0) (3.0.6)\n",
      "Collecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.2 MB 763 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (4.50.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (20.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (1.19.2)\n",
      "Requirement already satisfied: setuptools in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: jinja2 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (2.0.3)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (2.24.0)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->ru-core-news-sm==3.0.0) (2020.6.20)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=e11a7563b7f62d70207d7039036a6f666fbaaa49081798be43159192f7cf256d\n",
      "  Stored in directory: /Users/Andrew/Library/Caches/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "Successfully built docopt\n",
      "Installing collected packages: dawg-python, docopt, pymorphy2-dicts-ru, pymorphy2, ru-core-news-sm\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 ru-core-news-sm-3.0.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG Apple’s\n",
      "ORG Uber\n",
      "PERSON Tim Cook\n",
      "ORG Apple\n",
      "CARDINAL Millions\n",
      "ORG Uber’s\n",
      "LOC Silicon Valley’s\n",
      "ORG Yahoo\n",
      "PERSON Marissa Mayer\n",
      "MONEY 186\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(data)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-8e41a3480f27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "print(len(text[:1000000]))\n",
    "nlp.max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying fake news using supervised learning with NLP\n",
    "**Form of machine learning**\n",
    "- Problem has predefined training data\n",
    "- This data has a label what you want the model to learn\n",
    "- An example of supervised learning: classification problem, so we want to categorize some data based on what we already know "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning with NLP\n",
    "- Need to use language instead of geometric features\n",
    "- How to create supervised learning data from text\n",
    "    - We can use bag-of-words models or tf-idf as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning steps:\n",
    "- Collect and preprocess data\n",
    "- Determine a label (Example: Is the news fake?)\n",
    "- Split data into training and test sets\n",
    "- Extract features from the text to predict the model \n",
    "- Evaluate trained model on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So let's explore and apply the model to our data for classifying news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Firstly we will load our data, libraries and explore some contents of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv('News_articles/fake_or_real_news.csv')\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6335, 4)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8476, 'You Can Smell Hillary’s Fear',\n",
       "        'Daniel Greenfield, a Shillman Journalism Fellow at the Freedom Center, is a New York writer focusing on radical Islam. \\nIn the final stretch of the election, Hillary Rodham Clinton has gone to war with the FBI. \\nThe word “unprecedented” has been thrown around so often this election that it ought to be retired. But it’s still unprecedented for the nominee of a major political party to go war with the FBI. \\nBut that’s exactly what Hillary and her people have done. Coma patients just waking up now and watching an hour of CNN from their hospital beds would assume that FBI Director James Comey is Hillary’s opponent in this election. \\nThe FBI is under attack by everyone from Obama to CNN. Hillary’s people have circulated a letter attacking Comey. There are currently more media hit pieces lambasting him than targeting Trump. It wouldn’t be too surprising if the Clintons or their allies were to start running attack ads against the FBI. \\nThe FBI’s leadership is being warned that the entire left-wing establishment will form a lynch mob if they continue going after Hillary. And the FBI’s credibility is being attacked by the media and the Democrats to preemptively head off the results of the investigation of the Clinton Foundation and Hillary Clinton. \\nThe covert struggle between FBI agents and Obama’s DOJ people has gone explosively public. \\nThe New York Times has compared Comey to J. Edgar Hoover. Its bizarre headline, “James Comey Role Recalls Hoover’s FBI, Fairly or Not” practically admits up front that it’s spouting nonsense. The Boston Globe has published a column calling for Comey’s resignation. Not to be outdone, Time has an editorial claiming that the scandal is really an attack on all women. \\nJames Carville appeared on MSNBC to remind everyone that he was still alive and insane. He accused Comey of coordinating with House Republicans and the KGB. And you thought the “vast right wing conspiracy” was a stretch. \\nCountless media stories charge Comey with violating procedure. Do you know what’s a procedural violation? Emailing classified information stored on your bathroom server. \\nSenator Harry Reid has sent Comey a letter accusing him of violating the Hatch Act. The Hatch Act is a nice idea that has as much relevance in the age of Obama as the Tenth Amendment. But the cable news spectrum quickly filled with media hacks glancing at the Wikipedia article on the Hatch Act under the table while accusing the FBI director of one of the most awkward conspiracies against Hillary ever. \\nIf James Comey is really out to hurt Hillary, he picked one hell of a strange way to do it. \\nNot too long ago Democrats were breathing a sigh of relief when he gave Hillary Clinton a pass in a prominent public statement. If he really were out to elect Trump by keeping the email scandal going, why did he trash the investigation? Was he on the payroll of House Republicans and the KGB back then and playing it coy or was it a sudden development where Vladimir Putin and Paul Ryan talked him into taking a look at Anthony Weiner’s computer? \\nEither Comey is the most cunning FBI director that ever lived or he’s just awkwardly trying to navigate a political mess that has trapped him between a DOJ leadership whose political futures are tied to Hillary’s victory and his own bureau whose apolitical agents just want to be allowed to do their jobs. \\nThe only truly mysterious thing is why Hillary and her associates decided to go to war with a respected Federal agency. Most Americans like the FBI while Hillary Clinton enjoys a 60% unfavorable rating. \\nAnd it’s an interesting question. \\nHillary’s old strategy was to lie and deny that the FBI even had a criminal investigation underway. Instead her associates insisted that it was a security review. The FBI corrected her and she shrugged it off. But the old breezy denial approach has given way to a savage assault on the FBI. \\nPretending that nothing was wrong was a bad strategy, but it was a better one that picking a fight with the FBI while lunatic Clinton associates try to claim that the FBI is really the KGB. \\nThere are two possible explanations. \\nHillary Clinton might be arrogant enough to lash out at the FBI now that she believes that victory is near. The same kind of hubris that led her to plan her victory fireworks display could lead her to declare a war on the FBI for irritating her during the final miles of her campaign. \\nBut the other explanation is that her people panicked. \\nGoing to war with the FBI is not the behavior of a smart and focused presidential campaign. It’s an act of desperation. When a presidential candidate decides that her only option is to try and destroy the credibility of the FBI, that’s not hubris, it’s fear of what the FBI might be about to reveal about her. \\nDuring the original FBI investigation, Hillary Clinton was confident that she could ride it out. And she had good reason for believing that. But that Hillary Clinton is gone. In her place is a paranoid wreck. Within a short space of time the “positive” Clinton campaign promising to unite the country has been replaced by a desperate and flailing operation that has focused all its energy on fighting the FBI. \\nThere’s only one reason for such bizarre behavior. \\nThe Clinton campaign has decided that an FBI investigation of the latest batch of emails poses a threat to its survival. And so it’s gone all in on fighting the FBI. It’s an unprecedented step born of fear. It’s hard to know whether that fear is justified. But the existence of that fear already tells us a whole lot. \\nClinton loyalists rigged the old investigation. They knew the outcome ahead of time as well as they knew the debate questions. Now suddenly they are no longer in control. And they are afraid. \\nYou can smell the fear. \\nThe FBI has wiretaps from the investigation of the Clinton Foundation. It’s finding new emails all the time. And Clintonworld panicked. The spinmeisters of Clintonworld have claimed that the email scandal is just so much smoke without fire. All that’s here is the appearance of impropriety without any of the substance. But this isn’t how you react to smoke. It’s how you respond to a fire. \\nThe misguided assault on the FBI tells us that Hillary Clinton and her allies are afraid of a revelation bigger than the fundamental illegality of her email setup. The email setup was a preemptive cover up. The Clinton campaign has panicked badly out of the belief, right or wrong, that whatever crime the illegal setup was meant to cover up is at risk of being exposed. \\nThe Clintons have weathered countless scandals over the years. Whatever they are protecting this time around is bigger than the usual corruption, bribery, sexual assaults and abuses of power that have followed them around throughout the years. This is bigger and more damaging than any of the allegations that have already come out. And they don’t want FBI investigators anywhere near it. \\nThe campaign against Comey is pure intimidation. It’s also a warning. Any senior FBI people who value their careers are being warned to stay away. The Democrats are closing ranks around their nominee against the FBI. It’s an ugly and unprecedented scene. It may also be their last stand. \\nHillary Clinton has awkwardly wound her way through numerous scandals in just this election cycle. But she’s never shown fear or desperation before. Now that has changed. Whatever she is afraid of, it lies buried in her emails with Huma Abedin. And it can bring her down like nothing else has.  ',\n",
       "        'FAKE'],\n",
       "       [10294,\n",
       "        'Watch The Exact Moment Paul Ryan Committed Political Suicide At A Trump Rally (VIDEO)',\n",
       "        'Google Pinterest Digg Linkedin Reddit Stumbleupon Print Delicious Pocket Tumblr \\nThere are two fundamental truths in this world: Paul Ryan desperately wants to be president. And Paul Ryan will never be president. Today proved it. \\nIn a particularly staggering example of political cowardice, Paul Ryan re-re-re-reversed course and announced that he was back on the Trump Train after all. This was an aboutface from where he was a few weeks ago. He had previously declared he would not be supporting or defending Trump after a tape was made public in which Trump bragged about assaulting women. Suddenly, Ryan was appearing at a pro-Trump rally and boldly declaring that he already sent in his vote to make him President of the United States. It was a surreal moment. The figurehead of the Republican Party dosed himself in gasoline, got up on a stage on a chilly afternoon in Wisconsin, and lit a match. . @SpeakerRyan says he voted for @realDonaldTrump : “Republicans, it is time to come home” https://t.co/VyTT49YvoE pic.twitter.com/wCvSCg4a5I \\n— ABC News Politics (@ABCPolitics) November 5, 2016 \\nThe Democratic Party couldn’t have asked for a better moment of film. Ryan’s chances of ever becoming president went down to zero in an instant. In the wreckage Trump is to leave behind in his wake, those who cravenly backed his campaign will not recover. If Ryan’s career manages to limp all the way to 2020, then the DNC will have this tape locked and loaded to be used in every ad until Election Day. \\nThe ringing endorsement of the man he clearly hates on a personal level speaks volumes about his own spinelessness. Ryan has postured himself as a “principled” conservative, and one uncomfortable with Trump’s unapologetic bigotry and sexism. However, when push came to shove, Paul Ryan – like many of his colleagues – turned into a sniveling appeaser. After all his lofty tak about conviction, his principles were a house of cards and collapsed with the slightest breeze. \\nWhat’s especially bizarre is how close Ryan came to making it through unscathed. For months the Speaker of the House refused to comment on Trump at all. His strategy seemed to be to keep his head down, pretend Trump didn’t exist, and hope that nobody remembered what happened in 2016. Now, just days away from the election, he screwed it all up. \\nIf 2016’s very ugly election has done any good it’s by exposing the utter cowardice of the Republicans who once feigned moral courage. A reality television star spit on them, hijacked their party, insulted their wives, and got every last one of them to kneel before him. What a turn of events. \\nFeatured image via Twitter',\n",
       "        'FAKE'],\n",
       "       [3608, 'Kerry to go to Paris in gesture of sympathy',\n",
       "        'U.S. Secretary of State John F. Kerry said Monday that he will stop in Paris later this week, amid criticism that no top American officials attended Sunday’s unity march against terrorism.\\n\\nKerry said he expects to arrive in Paris Thursday evening, as he heads home after a week abroad. He said he will fly to France at the conclusion of a series of meetings scheduled for Thursday in Sofia, Bulgaria. He plans to meet the next day with Foreign Minister Laurent Fabius and President Francois Hollande, then return to Washington.\\n\\nThe visit by Kerry, who has family and childhood ties to the country and speaks fluent French, could address some of the criticism that the United States snubbed France in its darkest hour in many years.\\n\\nThe French press on Monday was filled with questions about why neither President Obama nor Kerry attended Sunday’s march, as about 40 leaders of other nations did. Obama was said to have stayed away because his own security needs can be taxing on a country, and Kerry had prior commitments.\\n\\nAmong roughly 40 leaders who did attend was Israeli Prime Minister Benjamin Netanyahu, no stranger to intense security, who marched beside Hollande through the city streets. The highest ranking U.S. officials attending the march were Jane Hartley, the ambassador to France, and Victoria Nuland, the assistant secretary of state for European affairs. Attorney General Eric H. Holder Jr. was in Paris for meetings with law enforcement officials but did not participate in the march.\\n\\nKerry spent Sunday at a business summit hosted by India’s prime minister, Narendra Modi. The United States is eager for India to relax stringent laws that function as barriers to foreign investment and hopes Modi’s government will act to open the huge Indian market for more American businesses.\\n\\nIn a news conference, Kerry brushed aside criticism that the United States had not sent a more senior official to Paris as “quibbling a little bit.” He noted that many staffers of the American Embassy in Paris attended the march, including the ambassador. He said he had wanted to be present at the march himself but could not because of his prior commitments in India.\\n\\n“But that is why I am going there on the way home, to make it crystal clear how passionately we feel about the events that have taken place there,” he said.\\n\\n“And I don’t think the people of France have any doubts about America’s understanding of what happened, of our personal sense of loss and our deep commitment to the people of France in this moment of trauma.”',\n",
       "        'REAL'],\n",
       "       [10142,\n",
       "        \"Bernie supporters on Twitter erupt in anger against the DNC: 'We tried to warn you!'\",\n",
       "        \"— Kaydee King (@KaydeeKing) November 9, 2016 The lesson from tonight's Dem losses: Time for Democrats to start listening to the voters. Stop running the same establishment candidates. \\n— People For Bernie (@People4Bernie) November 9, 2016 If Dems didn't want a tight race they shouldn't have worked against Bernie. \\n— Walker Bragman (@WalkerBragman) November 9, 2016 \\nNew York Times columnist Paul Krugman, who was one of Hillary Clinton’s most outspoken surrogates during the contentious Democratic primary, blamed Clinton’s poor performance on Green Party candidate Jill Stein, who has so far received a negligible number of votes nationally, saying Stein was the Ralph Nader of 2016 in preventing a Clinton victory. The account @BerniesTeachers threw Krugman’s analysis back in his face. Your candidate was the issue. Take responsibility. https://t.co/KHyOuUSrFS \\n— Teachers for Bernie (@BerniesTeachers) November 9, 2016 \\nAna Navarro, a Republican who recently endorsed Hillary Clinton, summed up the preposterous nature of the 2016 presidential election in this tweet: GOP nominated the only damn candidate who could lose to Hillary Clinton. Democrats nominated the only damn candidate who could lose to Trump \\n— Ana Navarro (@ananavarro) November 9, 2016 \\nPopular left-wing Facebook page The Other 98%, which was pro-Sanders during the primary, responded to Trump’s surge by simply posting a meme of Sanders’ face with the text “All this could’ve been avoided. Thanks for nothing, DNC!” The meme has been shared almost 15,000 times in less than an hour: \\nPosted by The Other 98% on Tuesday, November 8, 2016 \\nWhile Bernie Sanders endorsed Hillary Clinton just before the Democratic National Convention in July, many of his supporters remained adamant in their refusal to support the DNC-anointed candidate, pointing to WikiLeaks’ revelations that top officials at the DNC had been working behind the scenes to tip the scales in Clinton’s favor by coordinating with media figures to circulate anti-Sanders narratives. \\nRather than attribute a potential Trump presidency to the GOP nominee’s perceived popularity among voters, the closeness of this election could be credited to Hillary Clinton’s unfavorable ratings. According to RealClearPolitics, anywhere between 51 and 57 percent of voters had a negative opinion of the Democratic nominee. \\nAs of 11 PM Eastern, Florida, Michigan, Pennsylvania, and Wisconsin remain too close to call. Clinton has 197 electoral votes to Trump’s 187. \\n\\nZach Cartwright is an activist and author from Richmond, Virginia. He enjoys writing about politics, government, and the media. Send him an email at [email protected]\",\n",
       "        'FAKE'],\n",
       "       [875, 'The Battle of New York: Why This Primary Matters',\n",
       "        'It\\'s primary day in New York and front-runners Hillary Clinton and Donald Trump are leading in the polls.\\n\\nTrump is now vowing to win enough delegates to clinch the Republican nomination and prevent a contested convention. But Sens.Ted Cruz, R-Texas, Bernie Sanders, D-Vt., and Ohio Gov. John Kasich and aren\\'t giving up just yet.\\n\\nA big win in New York could tip the scales for both the Republican and Democratic front-runners in this year\\'s race for the White House. Clinton and Trump have each suffered losses in recent contests, shifting the momentum to their rivals.\\n\\n\"We have won eight out of the last nine caucuses and primaries! Cheer!\" Sanders recently told supporters.\\n\\nWhile wins in New York for Trump and Clinton are expected, the margins of those victories are also important.\\n\\nTrump needs to capture more than 50 percent of the vote statewide if he wants to be positioned to win all of the state\\'s 95 GOP delegates. That would put him one step closer to avoiding a contested convention.\\n\\n\"We\\'ve got to vote and you know Cruz is way, way down in the polls,\" Trump urged supporters.\\n\\nMeanwhile, Sanders is hoping for a close race in the Empire State. A loss by 10 points means he\\'ll need to win 80 percent of the remaining delegates to clinch the nomination.\\n\\nDespite a predicted loss in New York, Cruz hasn\\'t lost momentum. He\\'s hoping to sweep up more delegates this weekend while he\\'s talking about how he can win in November.\\n\\n\"Because if I\\'m the nominee, we win the General Election,\" Cruz promised his supporters. \"We\\'re beating Hillary in the key swing states, we\\'re beating Hillary with Independents, we\\'re beating Hillary with young people.\"\\n\\nFor now, Cruz, Kasich, and Sanders have all moved on from New York to other states. Trump and Clinton are the only two staying in their home state to watch the results come in.',\n",
       "        'REAL']], dtype=object)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier\n",
    "- Naive Bayes model\n",
    "    - Commonly used for testing NLP classification problem\n",
    "    - Basis is probability\n",
    "- Given a particular piece of data, how likely is a particular outcome?\n",
    "- MultinomialNB from sklearn.naive_bayes suits the CountVectorizer because it expects integer inputs\n",
    "- MultinomialNB is also used for multiple label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We explored our data now we will apply sckit-learn models for preprocessing and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000billion', '000ft']\n"
     ]
    }
   ],
   "source": [
    "vector = CountVectorizer(stop_words='english')\n",
    "target = news.label\n",
    "values = news.text\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(values,target,test_size=0.3,random_state=53)\n",
    "\n",
    "count_train = vector.fit_transform(X_train)\n",
    "\n",
    "count_test = vector.transform(X_test)\n",
    "\n",
    "print(vector.get_feature_names()[:10])\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(count_train,y_train)\n",
    "\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "score = metrics.accuracy_score(y_test,pred)\n",
    "print(score)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test,pred,labels=['FAKE', 'REAL'])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000billion', '000ft']\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.04169599 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.03144782 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.01437699 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\",max_df=0.7)\n",
    " \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "print(tfidf_train.A[:5])\n",
    "\n",
    "nb_classifier.fit(tfidf_train,y_train)\n",
    "\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "score = metrics.accuracy_score(y_test,pred)\n",
    "print(score)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test,pred,labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.8858495528669121\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.9042609153077328\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.9011046817464492\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.8953182535507628\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.8921620199894792\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.8884797475013151\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.8826933193056287\n",
      "\n",
      "Alpha:  0.7000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew/jupyter/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.875854813256181\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.8695423461336139\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.8679642293529721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "alphas = np.arange(0,1,0.1)\n",
    "\n",
    "def train_and_predict(alpha):\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "\n",
    "    nb_classifier.fit(tfidf_train,y_train)\n",
    "\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test,pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE [(-13.859494906966578, '0000'), (-13.859494906966578, '000035'), (-13.859494906966578, '0001'), (-13.859494906966578, '0001pt'), (-13.859494906966578, '000billion'), (-13.859494906966578, '000km'), (-13.859494906966578, '0011'), (-13.859494906966578, '006s'), (-13.859494906966578, '007'), (-13.859494906966578, '007s'), (-13.859494906966578, '008s'), (-13.859494906966578, '0099'), (-13.859494906966578, '00am'), (-13.859494906966578, '00p'), (-13.859494906966578, '00pm'), (-13.859494906966578, '013c2812c9'), (-13.859494906966578, '014'), (-13.859494906966578, '015'), (-13.859494906966578, '01am'), (-13.859494906966578, '020')]\n",
      "REAL [(-6.175170839285423, 'republicans'), (-6.123187810418293, 'percent'), (-6.113194244735139, 'political'), (-6.06755895002852, 'house'), (-5.989328960496733, 'like'), (-5.985516527362077, 'just'), (-5.9696611555722825, 'states'), (-5.965922833461676, 'sanders'), (-5.961826756239672, 'time'), (-5.841198768451058, 'party'), (-5.731795054148806, 'republican'), (-5.622544858905123, 'campaign'), (-5.57070909653965, 'new'), (-5.517369643632987, 'obama'), (-5.512852816745379, 'state'), (-5.477434389541838, 'president'), (-5.458835531806292, 'people'), (-4.985746778812183, 'clinton'), (-4.596562084871225, 'trump'), (-4.477472713300786, 'said')]\n"
     ]
    }
   ],
   "source": [
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0],feature_names))\n",
    "\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
